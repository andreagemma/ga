"""
This type stub file was generated by pyright.
"""

"""python-snappy

Python library for the snappy compression library from Google.
Expected usage like:

    import snappy

    compressed = snappy.compress("some data")
    assert "some data" == snappy.uncompress(compressed)

"""
_CHUNK_MAX = ...
_STREAM_TO_STREAM_BLOCK_SIZE = ...
_STREAM_IDENTIFIER = ...
_IDENTIFIER_CHUNK = ...
_STREAM_HEADER_BLOCK = ...
_compress = ...
_uncompress = ...
class UncompressError(Exception):
    ...


def isValidCompressed(data): # -> bool:
    ...

def compress(data, encoding=...): # -> bytes:
    ...

def uncompress(data, decoding=...): # -> str | bytes:
    ...

decompress = ...
class StreamCompressor:
    """This class implements the compressor-side of the proposed Snappy framing
    format, found at

        http://code.google.com/p/snappy/source/browse/trunk/framing_format.txt
            ?spec=svn68&r=71

    This class matches the interface found for the zlib module's compression
    objects (see zlib.compressobj), but also provides some additions, such as
    the snappy framing format's ability to intersperse uncompressed data.

    Keep in mind that this compressor object does no buffering for you to
    appropriately size chunks. Every call to StreamCompressor.compress results
    in a unique call to the underlying snappy compression method.
    """
    def __init__(self) -> None:
        ...
    
    def add_chunk(self, data: bytes, compress=...): # -> bytes:
        """Add a chunk, returning a string that is framed and compressed. 
        
        Outputs a single snappy chunk; if it is the very start of the stream,
        will also contain the stream header chunk.
        """
        ...
    
    compress = ...
    def flush(self): # -> bytes:
        ...
    
    def copy(self): # -> Self:
        """This method exists for compatibility with the zlib compressobj.
        """
        ...
    


class StreamDecompressor:
    """This class implements the decompressor-side of the proposed Snappy
    framing format, found at

        http://code.google.com/p/snappy/source/browse/trunk/framing_format.txt
            ?spec=svn68&r=71

    This class matches a subset of the interface found for the zlib module's
    decompression objects (see zlib.decompressobj). Specifically, it currently
    implements the decompress method without the max_length option, the flush
    method without the length option, and the copy method.
    """
    def __init__(self) -> None:
        ...
    
    @staticmethod
    def check_format(fin): # -> Literal[False]:
        """Does this stream start with a stream header block?

        True indicates that the stream can likely be decoded using this class.
        """
        ...
    
    def decompress(self, data: bytes): # -> bytes:
        """Decompress 'data', returning a string containing the uncompressed
        data corresponding to at least part of the data in string. This data
        should be concatenated to the output produced by any preceding calls to
        the decompress() method. Some of the input data may be preserved in
        internal buffers for later processing.
        """
        ...
    
    def flush(self): # -> bytes:
        ...
    
    def copy(self): # -> Self:
        ...
    


class HadoopStreamCompressor:
    def add_chunk(self, data: bytes, compress=...): # -> bytes:
        """Add a chunk, returning a string that is framed and compressed. 
        
        Outputs a single snappy chunk; if it is the very start of the stream,
        will also contain the stream header chunk.
        """
        ...
    
    compress = ...
    def flush(self): # -> Literal[b""]:
        ...
    
    def copy(self): # -> Self:
        """This method exists for compatibility with the zlib compressobj.
        """
        ...
    


class HadoopStreamDecompressor:
    def __init__(self) -> None:
        ...
    
    @staticmethod
    def check_format(fin): # -> bool:
        """Does this look like a hadoop snappy stream?
        """
        ...
    
    def decompress(self, data: bytes): # -> bytes:
        """Decompress 'data', returning a string containing the uncompressed
        data corresponding to at least part of the data in string. This data
        should be concatenated to the output produced by any preceding calls to
        the decompress() method. Some of the input data may be preserved in
        internal buffers for later processing.
        """
        ...
    
    def flush(self): # -> Literal[b""]:
        ...
    
    def copy(self): # -> Self:
        ...
    


def stream_compress(src, dst, blocksize=..., compressor_cls=...): # -> None:
    """Takes an incoming file-like object and an outgoing file-like object,
    reads data from src, compresses it, and writes it to dst. 'src' should
    support the read method, and 'dst' should support the write method.

    The default blocksize is good for almost every scenario.
    """
    ...

def stream_decompress(src, dst, blocksize=..., decompressor_cls=..., start_chunk=...): # -> None:
    """Takes an incoming file-like object and an outgoing file-like object,
    reads data from src, decompresses it, and writes it to dst. 'src' should
    support the read method, and 'dst' should support the write method.

    The default blocksize is good for almost every scenario.
    :param decompressor_cls: class that implements `decompress` method like
        StreamDecompressor in the module
    :param start_chunk: start block of data that have already been read from
        the input stream (to detect the format, for example)
    """
    ...

def hadoop_stream_decompress(src, dst, blocksize=...): # -> None:
    ...

def hadoop_stream_compress(src, dst, blocksize=...): # -> None:
    ...

def raw_stream_decompress(src, dst): # -> None:
    ...

def raw_stream_compress(src, dst): # -> None:
    ...

